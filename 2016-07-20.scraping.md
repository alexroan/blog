# Web Scraping

Web scraping is an extremely powerful tool for gathering information where an API is not available. I've recently been experimenting with different sorts of scraping tools and have come across some discussion about its legality. Is it legal? How legal? When is the line crossed? How do mainstream media and the courts around the world view it?

## Legal Issues

The legality of web scraping is somewhat in a state of flux. As far as I, and many other scrapers, are concerned: If it's public it's fair game (provided that the process of scraping does not hinder the origin server in any way, through throttling or delaying requests). Scraping in this scenario is no different to accessing the pages of a site with a web browser (albeit rather quickly). 

When the data is private, such as a Facebook news feed data where the scraper needs user login details to access this information on behalf of the user, the water gets far murkier. The general rule of thumb here is: don't do it. Pablo Hoffman of [Scrapy](http://scrapy.org/) (more on this later), on this issue says: 
> "Social networks, for example, assign the value of becoming a user (based on call-to-action on public page), as the ability to: i) Gain access to full profiles, ii) Identify common friends/connections, iii) Get introduced to others, and iv) Contact members directly. As long as scrapers makes no attempt to perform any of these actions they do not gain "unauthorized access" to their services and thus does not violate CFAA".

As well as this he states that, with regards to public data, simply putting a small link at the bottom of a page to the site's terms and conditions does not constitute a binding agreement between the website and the reader. Therefore any rules stated in the terms and conditions regarding the accessing of public information are unenforceable.

## Methods of Scraping

Python is widely recognised as the one stop shop for all your scraping needs. The basic libraries needed to start your first web scraper are: 

- [Requests](http://docs.python-requests.org/en/master/) - For making http requests
- [Mechanise](http://wwwsearch.sourceforge.net/mechanize/) - A programmatic web browser
- [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/) - A powerful markup parser

These three alone provide enough functionality to get a decent scraper off the ground. There are many other libraries which will aid more complex scraping, I recommend taking a look at [Cookielib](https://docs.python.org/2.7/library/cookielib.html) which can be used to manage cookies.

So you've made your first web scraper, awesome. It works but it took some time and looks very ugly. The next step is to scale, which is where [Scrapy](http://scrapy.org/) comes in. Scrapy provides a framework in which boilerplate code and logical structures and pipelines are provided. It takes a bit of getting used to, but once you have, maintaining scrapers is a hell of a lot easier than creating each scraper from scratch.

For a start, input and output is all handled by default, so there's no need to worry about where to start and finish. You can set the domains that are allowed, the rules in which to follow URLs, parser methods for certain types of URLs, and lots more. In terms of output: you set the objects that you want to create from data being scraped and these objects are printed as json by default. What's more is that the output can be directed to a number of different formats. 

### Storage

Once you've set up a web scraper, you'll need somewhere to store the data. You could store them in files (although I'm not quite sure why you would), or you could store them in a database. When I was first scraping, I'd create a local Apache server with a simple MySQL database in which to store the scraped data. This however, means you have to know exactly what data you're scraping, and create and maintain bespoke tables which adhere to the structure of what you're scraping. The problem being that it is very rigid. 

Solution: [Elasticsearch](https://github.com/elastic/elasticsearch). 

Elasticsearch is a distributed, scalable, flexible and very accessible tool which enables the storage and analysis of any type of data structure through a sophisticated REST API. Brand new, unknown objects can be thrown at a brand new endpoint using HTTP requests and elasticsearch knows how to handle it. It then stores and indexes the data so that it is highly searchable. In my opinion it's quite a game changer when it comes to data analysis and searching

## Summary

Scraping is too valuable a tool to be swept under the rug. Public data is public data. As long as scrapers are responsible, there is no issue with web scraping whatsoever. And if you're on the other side of it and don't want to get scraped, provide an API! It is public data after all, why not take the hassle away from your users and just provide structured data ready made. It may even become a new revenue stream if you play your cards right. Developers can develop on top of the API you provide and bring in more users to your service.

Information is power, and Web Scraping is just one of the many methods to gather information.